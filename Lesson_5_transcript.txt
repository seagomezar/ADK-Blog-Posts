0:03
In this lesson, you'll learn production grade multi-agent patterns
0:06
where you'll refactor your system into a three-tier architecture
0:09
with proper separation of concerns.
0:12
creating a scalable foundation for complex applications. Let's dive into the code.
0:16
Over the past few lessons, we
0:18
have progressively built a sophisticated research agent.
0:21
We started with a simple live voice agent
0:24
evolved it into a silent background worker
0:27
and then added robust guard rails using callbacks.
0:29
Now we assemble all those pieces
0:32
into a complete end-to-end content production pipeline.
0:35
Our goal is no longer just to create a research report,
0:38
it's to take a user request and automatically generate
0:41
a finished two-person audio podcast.
0:43
To achieve this, we are introducing three major advancements: a multi-agent
0:47
architecture, robust data structuring with Pydantic,
0:51
and a new tool that brings
0:52
a project into the world of audio.
0:54
Before we get on to the
0:56
code, let's quickly talk about the architecture.
0:58
We're now moving into a multi-agent system.
1:01
Our root agent, which we can now think of as a producer,
1:04
will orchestrate the entire workflow.
1:06
However, for the specialized task of audio generation, it'll delegate
1:10
the work to a new dedicated agent, the podcaster agent.
1:14
This is made possible by the AgentTool, which allows
1:17
one agent to be used as a tool by another.
1:19
This pattern of using specialized agent for a specific task
1:23
is fundamental to building complex and scalable systems.
1:26
Now let's start building the code and before we start,
1:29
as you know, we always create our folder.
1:31
Let's do that quickly. Now, let's
1:34
go to the code block by block.
1:37
The first thing that we see is the new two classes
1:39
at the top of our agent.py, which is NewsStory
1:42
and AINewsReport.
1:43
This is a significant upgrade from our previous method.
1:46
Instead of putting a Markdown schema in the prompt,
1:48
we are now using Pydantic schemas.
1:50
Think of this as a strict contract for our data.
1:53
The NewsStory class defines exactly what information we need for each article,
1:57
like the company, ticker, and a new field,
1:59
why it matters. The AINewsReport class then
2:02
acts as a container for the whole report,
2:05
holding a title, a summary, and
2:07
list of all these news story objects.
2:08
This ensures a data is always clean, structured, and predictable.
2:12
All right, our next action is to add our tools.
2:15
The very first two tools that we're going to create is tools
2:18
that are going to be used for creating and generating the podcast.
2:22
The first one is the wave file, which will save the output
2:25
that is coming from Gemini TTS to
2:27
a local file folder with a .wav format.
2:30
Next is our most exciting new tool, which is the generate_podcast_audio.
2:34
This function is a gateway to multimodal output.
2:37
It takes the text script as an input and uses
2:39
the Gemini text to speech model to convert it into audio.
2:43
The key feature here is the multi_speaker_voice_config.
2:46
We've defined two distinct speakers, Joe and Jane, with different prebuilt voices.
2:51
This allows the model to generate a natural
2:54
sounding conversational podcast, not just a monotonous reading.
2:57
The function then takes a raw audio data from the API.
3:00
and saves it as a .wav file in our project directory.
3:04
The most important element for this is gemini-2.5-flash-preview-tts model,
3:09
which are the text to speech models
3:11
we are using for this use case.
3:13
Now, let's add this into our agent.py.
3:15
Next we have the same get_financial_context and save_news_to_markdown
3:19
as you've seen in the previous lesson. So we'll quickly run them.
3:22
Now that we have added tools, the
3:24
next step is to actually add our callbacks,
3:26
which we have seen in the previous lesson as well. You'll recognize
3:29
filter_news_sources_callback which we had in the previous lesson,
3:32
and we've added one more which is enforce_data_freshness_callback.
3:35
There's another before tool callback.
3:37
Its job is very simple but crucial.
3:39
It intercepts the google_search call
3:41
and adds a parameter to the query
3:44
that restricts results to the past week,
3:46
ensuring our podcast is always based on current events.
3:49
Now, the second callback is inject_process_log_after_search and it's an after-tool callback.
3:55
This one is very clever. It runs after
3:58
the search is complete, it looks at the result,
4:01
figures out which of our whitelisted domains were actually used,
4:04
and then modifies the tool response.
4:06
Instead of just returning the search result as a string,
4:09
it returns a dictionary containing both the result and a process log.
4:13
This makes the invisible work of our
4:15
before tool callback visible to the agent,
4:18
which we then instructed to include
4:20
in the final report for full transparency.
4:23
We have the tools and we have the callback.
4:25
Now, the last missing piece
4:27
is essentially adding the agents.
4:29
So let's add our agent.
4:30
Now what you see here are two agents,
4:32
podcaster_agent and the root_agent.
4:34
This is our first multi-agent system.
4:36
The podcaster_agent is a specialist.
4:38
It has one job and one tool, which is to generate audio.
4:41
The root_agent is the producer.
4:43
By wrapping the podcaster_agent in an AgentTool,
4:46
we give our producer the ability to delegate the task
4:48
of audio generation to the specialist,
4:51
which is a much cleaner and more scalable design.
4:53
Finally, all of this comes together
4:56
in a new instruction of our root_agent.
4:58
Its identity is now an AI News podcast producer.
5:02
Its workflow is complete, 10-step production pipeline
5:05
from acknowledging the user's request to generating the final audio.
5:09
Crucially, we now connect all the pieces.
5:12
We tell it to use AINewsReport schema for its output
5:15
by setting output_schema equals to AINewsReport.
5:18
We instructed on how to handle the new dictionary output
5:21
from the callback modified Google search tool. We add a new create
5:24
step which is to create podcast script
5:26
where it must write a natural conversation
5:28
based on its finding and the final step
5:31
in its background work is to call the podcaster_agent tool.
5:35
When you run this final agent, the
5:36
user experience is as simple as ever.
5:38
But in the background, this sophisticated system of agents, tools, schemas,
5:42
and callbacks will execute, delivering not just a research report,
5:46
but the finished audio podcast. So let's get into it.
5:49
Now, let's start a server and run our ADK web.
5:53
As we have seen earlier, we can
5:55
directly access the UI with this link.
5:58
Now that our ADK web UI
5:59
is running, let's ask for a podcast.
6:01
Hi, can you help me get
6:02
the latest AI news in a podcast?
6:04
Okay, I'll start researching the latest
6:06
AI news for NASDAQ listed US companies.
6:09
I will enrich the findings with financial data where available
6:12
and compile a report for you. This might take
6:14
a moment. Now you saw that we were able to
6:16
talk to our agent and it told us
6:18
that it'll do all the things in the background
6:20
and voila, it actually did save the
6:23
report as well as the final podcast.
6:25
Let's look into it. Here we are actually reading the ai_research_report.md file
6:29
that got generated by the background process that our agent did.
6:33
As you can see, this is very detailed report.
6:35
It has all the listed companies that we asked for, it also
6:39
has the financial context.
6:41
It has the domains from where it is picking.
6:43
It has both the headline and the summary of why it matters.
6:47
This is really interesting. But the most interesting part is the podcast,
6:52
which is what we have been waiting
6:54
for ever since we started from lesson one.
6:57
And I'm not going to say much about it.
6:59
I would like you to listen to what it has produced.
7:02
Podcast script, AI News roundup.
7:08
Hey everyone, and welcome to the AI Today podcast.
7:11
I'm your enthusiastic host, Joe.
7:14
And I'm Jane, here to provide the analytical perspective
7:16
on today's AI developments.
7:18
Today, we're diving into the latest AI news
7:21
from NASDAQ listed US companies.
7:23
Jane, what's catching your eye?
7:26
Well, Joe, it's clear that AI is rapidly transforming
7:28
the tech landscape. Let's start with Microsoft.
7:32
Amazing. This is what we have been wanting to accomplish
7:35
and just with few blocks of code, we get
7:38
two-person podcast based on our requirements. This is just mind-blowing.
7:41
And we have come a long way from a simple
7:44
agent that was updating us about live news to this,
7:46
where we have a quick podcast about the latest news.
7:50
To achieve this, we had to
7:51
learn so many different elements of ADK
7:54
and I can't really wait for you to find more out-of-the-box ideas
7:58
to build with ADK and Gemini Live model. Happy building.
8:32
To achieve this we had to learn many different elements of a K.
8:35
and I can't wait for you to find more out of the box ideas
8:39
to build with a D, K, and Gemini life marker.
8:42
Happy building!
